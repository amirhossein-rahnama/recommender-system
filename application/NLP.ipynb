{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "data = pd.read_csv('data_cut.csv')\n",
    "\n",
    "\n",
    "tokenized_lyrics = []\n",
    "pos_tags = []\n",
    "named_entities = []\n",
    "\n",
    "\n",
    "for index, row in data[data['language'] == 'en'].iterrows():\n",
    "    lyrics = row['lyrics']\n",
    "\n",
    "\n",
    "    doc = nlp(lyrics)\n",
    "\n",
    "    tokens = [token.text for token in doc]\n",
    "    tokenized_lyrics.append(tokens)\n",
    "\n",
    " \n",
    "    pos = [f\"{token.text} ({token.pos_})\" for token in doc]\n",
    "    pos_tags.append(pos)\n",
    "\n",
    "\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    named_entities.append(entities)\n",
    "\n",
    "\n",
    "data.loc[data['language'] == 'en', 'tokenized_lyrics'] = tokenized_lyrics\n",
    "data.loc[data['language'] == 'en', 'pos_tags'] = pos_tags\n",
    "data.loc[data['language'] == 'en', 'named_entities'] = named_entities\n",
    "\n",
    "\n",
    "data.to_csv('data_process.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('final_dataset_no_duplicates.csv')\n",
    "df2 = pd.read_csv('add.csv')\n",
    "\n",
    "\n",
    "concatenated_df = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "\n",
    "if 'Unnamed: 0' in concatenated_df.columns:\n",
    "    concatenated_df = concatenated_df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "\n",
    "concatenated_df.to_csv('concatenated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "df = pd.read_csv('concatenated.csv')\n",
    "\n",
    "\n",
    "df['lyrics'] = df['lyrics'].fillna(\"\")\n",
    "\n",
    "df['sentiment'] = df['lyrics'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "\n",
    "data = df['lyrics'].tolist()\n",
    "train_corpus = [TaggedDocument(words=word_tokenize(text.lower()), tags=[str(i)]) for i, text in enumerate(data)]\n",
    "model = Doc2Vec(vector_size=50, min_count=2, epochs=40, dm=1)\n",
    "\n",
    "\n",
    "model.build_vocab(train_corpus)\n",
    "\n",
    "\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "model.save('trained_model_example')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Doc2Vec.load('trained_model_example')\n",
    "\n",
    "emb_df = pd.DataFrame([model.dv[str(i)] for i in range(len(data))])\n",
    "\n",
    "\n",
    "fe_df = pd.concat([df, emb_df], axis=1)\n",
    "\n",
    "\n",
    "print(fe_df.head())\n",
    "\n",
    "\n",
    "fe_df.to_csv('data_with_vectors_and_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def sentiment_analysis(text, sentiment_pipeline, chunk_size=512):\n",
    "    \n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    \n",
    "    sentiments = []\n",
    "    try:\n",
    "        for chunk in chunks:\n",
    "            chunk_sentiment = sentiment_pipeline(chunk)[0]['label']\n",
    "            sentiments.append(chunk_sentiment)\n",
    "        \n",
    "        \n",
    "        most_common_sentiment = max(set(sentiments), key=sentiments.count)\n",
    "        return most_common_sentiment\n",
    "    except Exception as e:\n",
    "        print(f\"Error in sentiment analysis: {e}\")\n",
    "        return \"error\"\n",
    "\n",
    "\n",
    "sentiment_model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=sentiment_model, tokenizer=sentiment_tokenizer)\n",
    "\n",
    "\n",
    "df = pd.read_csv('concatenated.csv')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    \n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    \n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    \n",
    "    patterns = [(r\"(\\w)\\1{2,}\", r\"\\1\\1\"), (r\"(\\w)-\\1+\", r\"\\1\"), (r\"(\\b\\w+\\b)-\\1+\", r\"\\1\")]\n",
    "    text = reduce(lambda doc, pattern: re.sub(pattern[0], pattern[1], doc), patterns, contractions.fix(text.lower()))\n",
    "\n",
    "    \n",
    "    word_counts = Counter(word_tokenize(text))\n",
    "    text = ' '.join([word if word_counts[word] <= 10 else '' for word in text.split()])\n",
    "\n",
    "    \n",
    "    symbols = set(string.punctuation).union({\"`\", \"“\", \"”\", \"‘\", \"’\", \"–\", \"—\"})\n",
    "    text = \" \".join(word for word in word_tokenize(text) if word not in symbols)\n",
    "\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    \n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    final_text = \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['lyrics'] = df['lyrics'].fillna(\"\").apply(preprocess_text)\n",
    "\n",
    "\n",
    "df['sentiment'] = df['lyrics'].apply(lambda x: sentiment_analysis(x, sentiment_pipeline))\n",
    "\n",
    "\n",
    "embedding_model_name = \"xlm-roberta-base\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
    "\n",
    "\n",
    "def get_embeddings(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "\n",
    "df['embeddings'] = df['lyrics'].apply(lambda x: get_embeddings(x, embedding_tokenizer, embedding_model))\n",
    "\n",
    "\n",
    "def flatten_embeddings(embeddings):\n",
    "    return [np.array(embedding).flatten() for embedding in embeddings]\n",
    "\n",
    "\n",
    "df['flattened_embeddings'] = flatten_embeddings(df['embeddings'].tolist())\n",
    "\n",
    "\n",
    "df.to_csv('enhanced_data_with_sentiment_and_embeddings.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv('enhanced_data_with_sentiment_and_embeddings.csv')\n",
    "\n",
    "\n",
    "def convert_string_to_list(string):\n",
    "    try:\n",
    "        \n",
    "        return np.fromstring(string.strip('[]'), sep=' ')\n",
    "    except ValueError:\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "df['flattened_embeddings'] = df['flattened_embeddings'].apply(convert_string_to_list)\n",
    "\n",
    "\n",
    "df = df.dropna(subset=['flattened_embeddings'])\n",
    "\n",
    "\n",
    "embeddings_df = pd.DataFrame(df['flattened_embeddings'].tolist())\n",
    "\n",
    "\n",
    "embeddings_df.columns = [f'embedding_{i}' for i in range(embeddings_df.shape[1])]\n",
    "\n",
    "\n",
    "final_df = pd.concat([df, embeddings_df], axis=1)\n",
    "\n",
    "\n",
    "final_df.to_csv('enhanced_data_with_expanded_flattened_embeddings.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
